{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание 5: LLM и RAG\n",
        "\n",
        "В данной работе вам предстоит создать чат-бота-врача, используя метод Retrieval-Augmented Generation (RAG) и фреймворки Huggingface и LangChain.\n",
        "\n",
        "---\n",
        "**Содержание:**\n",
        "- Загрузка и подготовка данных MEDAL.\n",
        "- Чтение и индексация данных.\n",
        "- Создание эмбеддингов и векторного хранилища.\n",
        "- Построение LLM и настройка поиска.\n",
        "- Разработка шаблона prompt (Prompt Engineering).\n",
        "- Создание LangChain Pipeline.\n",
        "- Бонус: Интеграция истории переписки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install datasets langchain_community langchain_chroma langchain langchain_core tiktoken sentence-transformers==2.2.2 lark InstructorEmbedding bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Загрузка датасета MEDAL\n",
        "\n",
        "В этом разделе мы загружаем датасет [MEDAL](https://huggingface.co/datasets/bigbio/medal), содержащий медицинские статьи для различных клинических диагнозов.\n",
        "\n",
        "**Замечание:** Нас интересуют колонки `TEXT` и `LABEL`.\n",
        "\n",
        "Дополнительные ссылки:\n",
        "- [Репозиторий MEDAL](https://github.com/McGill-NLP/medal)\n",
        "- [train.csv (Zenodo)](https://zenodo.org/record/4482922/files/train.csv)\n",
        "- [Файл на Google Drive](https://drive.google.com/file/d/1X7PTIkmsFhTk5n-4W6SWa7XWsDGTpafl/view?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 0: Загрузка данных MEDAL (0.5 балла)\n",
        "\n",
        "Просмотр содержимого файла с данными:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ABSTRACT_ID,TEXT,LOCATION,LABEL\n",
            "14145090,velvet antlers vas are commonly used in traditional chinese medicine and invigorant and contain many PET components for health promotion the velvet antler peptide svap is one of active components in vas based on structural study the svap interacts with tgfÎ² receptors and disrupts the tgfÎ² pathway we hypothesized that svap prevents cardiac fibrosis from pressure overload by blocking tgfÎ² signaling SDRs underwent TAC tac or a sham operation T3 one month rats received either svap mgkgday or vehicle for an additional one month tac surgery induced significant cardiac dysfunction FB activation and fibrosis these effects were improved by treatment with svap in the heart tissue tac remarkably increased the expression of tgfÎ² and connective tissue growth factor ctgf ROS species C2 and the phosphorylation C2 of smad and ERK kinases erk svap inhibited the increases in reactive oxygen species C2 ctgf expression and the phosphorylation of smad and erk but not tgfÎ² expression in cultured cardiac fibroblasts angiotensin ii ang ii had similar effects compared to tac surgery such as increases in Î±smapositive CFs and collagen synthesis svap eliminated these effects by disrupting tgfÎ² IB to its receptors and blocking ang iitgfÎ² downstream signaling these results demonstrated that svap has antifibrotic effects by blocking the tgfÎ² pathway in CFs,63,transverse aortic constriction\n",
            "1900667,the clinical features of our cases demonstrated some of the already known characteristics of the variable spectrum of hiv infection da are the most important risk category in italy of the arc cases evolved into aids during a month followup on average the most frequent oi in our aids cases were pcp c albicans esophagitis and chronic mucocutaneous ulcers an high percentage of neurologic involvement from hiv was observed and HM were encountered in aids ks and undifferentiated b lymphoma as well as in arc HD statistically significant worsening of the immunologic situation is evident as the disease progresses from las to aids G1 b lymphocytes represent most of the cells of the germinal center during the hyperplastic stage of lymphadenopathy reversal of the tt ratio appears early during the initial stage of lymphadenopathy and is due to a decrease of cd and a relative increase of cd also destruction of the follicular dendritic cells is an early feature which becomes more evident as the disease advances and the lymph node evolves toward progressive involution activated blymphocyte augmentation with polyclonal ig secretion appears to be related to TI b stimulation by coinfection such as cmv ebv and hbv the increase of CD8 lymphocytes seems to be partly related to the excessive activation of b lymphocytes and partially directed to the cells INF by hiv or coated with its proteins the destruction of follicular dendritic cells has been interpreted not only as a killer effect of the virus but also as a result of the intervention of ctl sensitized to the cells containing the virus their destruction may contribute to the impaired recognition of soluble antigen which is one of the main features of the immune deficiency of hiv infection,85,hodgkins lymphoma\n",
            "8625554,ceftobiprole bpr is an investigational cephalosporin with activity against staphylococcus aureus including methicillinresistant s aureus mrsa strains the pharmacodynamic pd profile of bpr against s aureus strains with a variety of susceptibility phenotypes in an immunocompromised murine pneumonia model was characterized the bpr mics of the test isolates ranged from to mugml pharmacokinetic pk studies were conducted with infected neutropenic balbc mice and the bpr concentrations were measured in plasma epithelial lining fluid elf and lung tissue pd studies with these mice were undertaken with eight s aureus isolates two MSSA strains three hospitalacquired mrsa strains and three CA mrsa strains subcutaneous bpr doses of to mgkg of body weightday were administered and the NC in the number of log cfuml in lungs was evaluated after h of therapy the pd profile was characterized by using the free drug exposures f determined from the following parameters the percentage of time that the concentration was greater than the mic t mic the Cmax in serummic and the area under the concentrationtime curvemic the bpr pk parameters were linear over the dose range studied in plasma and the elf concentrations ranged from to of the free plasma concentration ft mic was the parameter that best correlated with tau against a diverse array of s aureus isolates in this mu pneumonia model the effective dose ed ed and stasis exposures appeared to be similar among the isolates studied bpr exerted maximal antibacterial effects when ft mic ranged from to regardless of the phenotypic profile of resistance to betalactam fluoroquinolone erythromycin clindamycin or TCs,90,methicillinsusceptible s aureus\n",
            "8157202,we have taken a basic biologic RPA to elucidate the pathophysiology of BPD bpd the CLD based on cellmolecular mechanisms of physiologic lung OD stretch coordinates PTHrP pthrp signaling between the ATII cell and the mesoderm to coordinately upregulate key genes for the homeostatic FB phenotype including peroxisome proliferator G1 TRG ppargamma adipocyte differentiation related protein adrp and OB and the retrograde stimulation of type ii cell surfactant synthesis by leptin each of these paracrine interactions requires cellspecific receptors on adjacent cells derived from the mesoderm or endoderm respectively to serially upregulate the signaling pathways between and within each celltype it is this PET compartmentation that is key to understanding how specific agonists and antagonists can predictably affect this mechanism of AM homeostasis using a wide variety of pathophysiologic insults associated with bpd barotrauma oxotrauma and infection we have found that there are type ii cell andor FB cellmolecular effects generated by these insults which can lead to the bpd phenotype we have exploited these cellspecific mechanisms to effectively prevent and treat lung injuries using ppargamma agonists to sustain this signaling pathway it is critically important to judiciously select physiologically and developmentally relevant interventions when treating the preterm neonate,26,parathyroid hormonerelated protein\n",
            "6784974,lipoperoxidationderived aldehydes for example MDA mda can damage proteins by generating covalent adducts whose accumulation probably participates in tissue damage during aging however the mechanisms of adduct formation and their stability are scarcely known this article investigates whether oxidative steps are involved in the process as a MM of the process the interaction between mda and bovine SS Al bsa was analyzed incubation of bsa with mda resulted in rapid quenching of tryptophan fluorescence and appearance of mda protein adduct fluorescence transition metal ion traces interfered with the latter process mda induced generation of peroxides in bsa which was preventable with the antioxidant BHT bht mdaexposed bsa underwent aggregation degradation and bhtsensitive gel retardation effects phycoerythrin fluorescence disappearance a marker of damage mediated by ROS species indicated synergism between mda and metal ions the interaction between reactive aldehydes and proteins is likely to occur in several steps some of them oxidative in nature giving rise to T3 LPO endproducts which could participate with AGEs in the generation of tissue damage during aging,157,lipoperoxidation\n",
            "4846741,gb virus c gbvc or HGV hgv is transmitted by the parenteral route but the importance of sexual transmission needs to be ascertained gbvchgv infections were investigated using rna and eantibody PCD methods in subjects INF by the HIV-1 hiv divided into groups of individuals each according to their main risk factor for hiv infection blood product recipients group intravenous drug users group homosexuals group or heterosexual SE group the overall prevalence of gbvchgv infection was no significant difference was observed in gbvc hgv prevalence among the four CG and in CG and respectively hepatitis c virus hcv antibodies used as a control for parenteral exposure were found in and of the subjects in groups and versus only and of the subjects in groups and respectively p similarly coinfections with gbvchgv and hcv were significantly associated with the parenteral route p these data emphasized the usefulness of combining the detection of rna and the e antibody to determine the actual prevalence of gbvchgv infection the high prevalence of the gbvchgv markers among the HIV+ subjects especially those with sexual exposure provides additional evidence that this route of transmission plays a key role in the epidemiology of gbvchgv the potential influence of gbvchgv infection on the course of hiv disease needs further DUE,5,hepatitis g virus\n",
            "14445409,to quantify the amount of CL NO harvested in modified RND,10,radical neck dissection\n",
            "9441271,backward SBS was used to control the growth of ASE ase reducing the unwanted emission in a pulseamplified cw tisapphire laser system from to less than x in the final output suppression of ase substantially improved the spectral SQ of the laser and broadened the range over which the laser is useful the SO duration was compressed but the pulse remained nearly transform limited,9,amplified spontaneous emission\n",
            "3689975,in order to study the rate of intestinal absorption and hepatic uptake of mediumchain fatty acids mcfa six growing pigs mean body weight kg were fitted with a permanent fistula in the duodenum and with three catheters in the portal vein i.c. and HV respectively two electromagnetic flow probes were also set up one around the portal vein and one around the hepatic artery a mixture of octanoic and decanoic acids esterified as mediumchain triacylglycerols together with maltose dextrine and a nitrogenous fraction was continuously infused for h into the duodenum samples of blood were withdrawn from the three vessels at regular intervals for h and further analysed for their nonesterified octanoic and decanoic acid contents the concentration of nonesterified octanoic and decanoic acids in the portal blood rose sharply after the beginning of each infusion and showed a biphasic timecourse with two maximum values one T3 min and a later one between and min only of octanoic acid infused into the duodenum and of C10 were recovered in the portal flow throughout each experiment the amounts of nonesterified mcfa taken up per h by the liver were close to those absorbed from the gut via the portal vein within the same periods of time showing that the liver is the main site of utilization of mcfa in pigs these results have been discussed with a special emphasis laid on the possible mechanisms of the biphasic timecourse of mcfa absorption and the incomplete REC in the HPB of the infused fatty acids,246,portal blood\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!head -n 10 train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задание 1: Чтение и индексация данных (2.5 балла)\n",
        "\n",
        "Здесь необходимо:\n",
        "- Прочитать данные из CSV файла.\n",
        "- Создать генератор документов с использованием метода `.lazy_load()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "FILE_PATH = 'medal.csv'\n",
        "docs = CSVLoader(FILE_PATH).load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Создание эмбеддингов и векторного хранилища\n",
        "\n",
        "На данном этапе:\n",
        "- Определите модель для вычисления эмбеддингов.\n",
        "- Создайте векторное хранилище для дальнейшего поиска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Инициализация модели эмбеддингов\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='mps')\n",
        "emb_model = HuggingFaceInstructEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    client=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Создание векторного хранилища (индекса)\n",
        "\n",
        "Настройте индекс для эмбеддингов с использованием Chroma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yl/bg_6c7_56kl413fpblgzt_5w0000gn/T/ipykernel_79430/745139480.py:7: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectordb.persist()\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = 'DB'\n",
        "\n",
        "# Создание индекса\n",
        "vectordb = Chroma.from_documents(documents=docs[:2000], embedding=emb_model, persist_directory=persist_directory)\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Индексация части документов\n",
        "\n",
        "Для ускорения работы проиндексируйте первые **N_DOCS** документов, так как полный датасет может содержать миллионы записей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04154f455eef45a38d768d96474bbede",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "N_DOCS = 2000  # Обработка первых 2000 документов\n",
        "\n",
        "for i, doc in tqdm(enumerate(docs[:2000]), total=N_DOCS):\n",
        "    vectordb.add_documents([doc])\n",
        "\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Проверка содержимого каталога с индексом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 90184\n",
            "-rw-r--r--  1 konstantin  staff    43M Apr 21 17:46 chroma.sqlite3\n",
            "drwxr-xr-x  7 konstantin  staff   224B Apr 21 17:42 \u001b[34ma8c83928-8903-4560-9d9f-276296fe9b2b\u001b[m\u001b[m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!ls -lht DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задание 2: Создание LLM и настройка поиска (2 балла)\n",
        "\n",
        "В этом разделе:\n",
        "- Настройте модель LLM для генерации ответов.\n",
        "- Проверьте работу поиска по индексу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_load_from_file_impl: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1637 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.24 B\n",
            "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device Metal\n",
            "load_tensors: layer   1 assigned to device Metal\n",
            "load_tensors: layer   2 assigned to device Metal\n",
            "load_tensors: layer   3 assigned to device Metal\n",
            "load_tensors: layer   4 assigned to device Metal\n",
            "load_tensors: layer   5 assigned to device Metal\n",
            "load_tensors: layer   6 assigned to device Metal\n",
            "load_tensors: layer   7 assigned to device Metal\n",
            "load_tensors: layer   8 assigned to device Metal\n",
            "load_tensors: layer   9 assigned to device Metal\n",
            "load_tensors: layer  10 assigned to device Metal\n",
            "load_tensors: layer  11 assigned to device Metal\n",
            "load_tensors: layer  12 assigned to device Metal\n",
            "load_tensors: layer  13 assigned to device Metal\n",
            "load_tensors: layer  14 assigned to device Metal\n",
            "load_tensors: layer  15 assigned to device Metal\n",
            "load_tensors: layer  16 assigned to device Metal\n",
            "load_tensors: layer  17 assigned to device Metal\n",
            "load_tensors: layer  18 assigned to device Metal\n",
            "load_tensors: layer  19 assigned to device Metal\n",
            "load_tensors: layer  20 assigned to device Metal\n",
            "load_tensors: layer  21 assigned to device Metal\n",
            "load_tensors: layer  22 assigned to device Metal\n",
            "load_tensors: layer  23 assigned to device Metal\n",
            "load_tensors: layer  24 assigned to device Metal\n",
            "load_tensors: layer  25 assigned to device Metal\n",
            "load_tensors: layer  26 assigned to device Metal\n",
            "load_tensors: layer  27 assigned to device Metal\n",
            "load_tensors: layer  28 assigned to device Metal\n",
            "load_tensors: layer  29 assigned to device Metal\n",
            "load_tensors: layer  30 assigned to device Metal\n",
            "load_tensors: layer  31 assigned to device Metal\n",
            "load_tensors: layer  32 assigned to device Metal\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4095.08 MiB, ( 4095.14 / 10922.67)\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors: Metal_Mapped model buffer size =  4095.07 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
            "..............................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2 Pro\n",
            "ggml_metal_init: picking default device: Apple M2 Pro\n",
            "ggml_metal_load_library: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has residency sets    = false\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x1620a9e90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row                                0x4fe030a20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x4fe031f40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row                                0x518febd50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x4fe0318f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row                                0x4fe0303e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x4fe0325e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row                                0x4fe033d80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x4fe0334f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x4fe034480 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x4fe035250 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x1620ae800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x1620afd60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x4fe035860 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x4fe0361b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x518fec8b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x4fe036a10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x4fe037320 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x1620b0d70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x4fe037c80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x4fde2e730 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x4fe038660 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x4fe038f70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x4fe03a1a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x4fe03abb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x4fe03b140 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x4fe03b480 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x518e14fb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x518c31f20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x518fece30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x518fee020 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x4fe03c530 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x4fe03ccd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x4fe03be80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x4fe03dba0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x161f04290 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x4fe03e590 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x4fe03f170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x4fe03f940 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1620b1060 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x4fe03ecd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x4fe040330 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1620b1b80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x4fe040b40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x4fde2e990 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x4fe0412e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x4fe0420e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1620b21c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x4fe041a90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x4fe042ff0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x518fee7e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x4fe043770 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x4fe042820 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x4fe0445c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x4fe043ea0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x518fef190 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x4fe044e30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1620b2c30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x4fe045d80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1620b3990 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x4fe045710 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x4fe045970 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x4fe047490 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x4fe047c50 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x4fe046c10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x4fe0465c0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x4fe048c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x4fe0492c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x4fe0499e0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1620b4800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x4fe04a850 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x4fe04a270 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x4fe04b1c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x4fe04b930 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x4fe04c810 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x518fef5c0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x161f04840 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x161f04aa0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1620b5890 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x4fe04d200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x161f04d00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x518ff0a10 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x4fe04e0f0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x4fe04db40 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x4fe04eac0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x4fe04f890 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x4fe04f2b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x4fe050250 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x4fe0508e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x518fefcc0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x4fe051830 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x4fe0511a0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x4fe052890 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x4fe0521e0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x4fe053180 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x4fe053890 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x4fe053af0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x4fe054200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x4fe054960 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x518ff1140 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x4fe055080 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x4fe056160 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x161f05510 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x4fe055bf0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1620b6e10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x4fe056c80 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1620b6720 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x4fe0573f0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x4fe057ee0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1620b62b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x4fe058560 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x161f05a70 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x4fe059280 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x4fd55d970 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x51909ff80 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x4fe058ad0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1620b8310 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x4fe05a460 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x4fe05b1e0 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x4fe05bfa0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x4fe05b6d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x4fe05cfa0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x518ff1a60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x4fe05c9b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x4fe05d620 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x161f05cd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x161f063f0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x4fe05d9c0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x4fe05e150 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1620b8850 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x4fe05f510 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x4fe05f000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x4fe060560 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x4fe05ff60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x4fe0615a0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x4fe061db0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x4fe062650 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1620b9ee0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x4fe060ee0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x4fe063630 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x4fde2ebf0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x161f068e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x4fe062fa0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x4fe0645c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x4fe063ea0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x4fe0653e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x4fe065bc0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1620bac00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x4fe066360 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x4fe064c80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x4fe064fb0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x4fe066d60 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x4fe0673d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x161f070a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x4fe0680c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x4fe067b20 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x4fe068b20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x4fe069230 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x4fe069980 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x4fe06a020 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11119e0d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x4fe06ac40 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1620b98f0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x4fe06a690 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1620bb520 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x4fe06c650 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1620bbbe0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x4fe06cea0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x4fe06b620 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x4fe06d8c0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x4fe06e640 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x4fe06dfc0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x4fe06efc0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x161f07430 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x4fe06f720 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x4fe06fe80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x4fe0700e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x518ff2140 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x4fe070860 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1620bd5d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x4fe071850 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x4fe071be0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x518ff3290 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x4fe0727e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x4fe0712f0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x4fe0736b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x518ff26c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x4fe073fa0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x4fe074d40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x4fe0745f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x4fe075bb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x4fe076540 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1620bdc30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x4fe076ec0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x4fe077710 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x4fe077f80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x4fe075530 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x4fe075790 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x4fe079660 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x4fde2ee50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x4fe078750 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x4fe07a630 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x4fe07ac80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x4fe07ba50 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x4fe07b3b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x4fe07c3a0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x4fe07c600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x4fe07cd00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x4fe07d810 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x4fe07db40 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x4fe07e310 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x4fe07eaa0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x4fe07f2a0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x4fe07f940 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1620befa0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x4fe080eb0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x4fe07ff40 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x4fe081aa0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4fe082b20 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x4fe081580 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x4fe083500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4fe083e60 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1620c0600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x4fe083bd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x4fe084a00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x4fe0854f0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1620c0c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x4fe085c60 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x4fe086420 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x4fde2f0b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x4fe086850 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x4fe086fd0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x4fe088460 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x518ff3ce0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x518ff4500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x4fe088dc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x4fe087e50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x4fe089de0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x4fe0897d0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x4fe08ade0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x4fde2f310 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x4fe08b610 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x161f07690 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x4fe08a790 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x4fe08c0c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1620c1c90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x4fde2f570 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x4fe08c8b0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x4fe08d630 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x4fd55dbd0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x161f083b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x5190a03c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x4fe08d160 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x161f078f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x518ff5950 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x4fd55de30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x4fe08e210 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x4fde2f7d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x4fe08f070 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x4fe08e950 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x4fe08ff20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x4fe08f690 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x4fe08f8f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x5190a0bd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x518a30d70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1620c2640 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x5190a1240 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x4fe090ca0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x4fe091f10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1620c3a50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1620c2e30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1620c3090 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x4fe0922f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x4fe0934f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x161f08e90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x4fe093b40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x4fde2fa30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x161f095b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x5190a14a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x4fe094bb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x161f09b70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x4fe095480 | th_max = 1024 | th_width =   32\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
            "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:      Metal compute buffer size =   296.00 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =    16.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 2\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_community.llms.llamacpp import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_batch=512,\n",
        "    n_ctx=4096,\n",
        "    f16_kv=True,\n",
        "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'row': 2, 'source': 'medal.csv'}, page_content=': 2\\nTEXT: ceftobiprole bpr is an investigational cephalosporin with activity against staphylococcus aureus including methicillinresistant s aureus mrsa strains the pharmacodynamic pd profile of bpr against s aureus strains with a variety of susceptibility phenotypes in an immunocompromised murine pneumonia model was characterized the bpr mics of the test isolates ranged from to mugml pharmacokinetic pk studies were conducted with infected neutropenic balbc mice and the bpr concentrations were measured in plasma epithelial lining fluid elf and lung tissue pd studies with these mice were undertaken with eight s aureus isolates two MSSA strains three hospitalacquired mrsa strains and three CA mrsa strains subcutaneous bpr doses of to mgkg of body weightday were administered and the NC in the number of log cfuml in lungs was evaluated after h of therapy the pd profile was characterized by using the free drug exposures f determined from the following parameters the percentage of time that the concentration was greater than the mic t mic the Cmax in serummic and the area under the concentrationtime curvemic the bpr pk parameters were linear over the dose range studied in plasma and the elf concentrations ranged from to of the free plasma concentration ft mic was the parameter that best correlated with tau against a diverse array of s aureus isolates in this mu pneumonia model the effective dose ed ed and stasis exposures appeared to be similar among the isolates studied bpr exerted maximal antibacterial effects when ft mic ranged from to regardless of the phenotypic profile of resistance to betalactam fluoroquinolone erythromycin clindamycin or TCs\\nLABEL: methicillinsusceptible s aureus'),\n",
              " Document(metadata={'row': 2, 'source': 'medal.csv'}, page_content=': 2\\nTEXT: ceftobiprole bpr is an investigational cephalosporin with activity against staphylococcus aureus including methicillinresistant s aureus mrsa strains the pharmacodynamic pd profile of bpr against s aureus strains with a variety of susceptibility phenotypes in an immunocompromised murine pneumonia model was characterized the bpr mics of the test isolates ranged from to mugml pharmacokinetic pk studies were conducted with infected neutropenic balbc mice and the bpr concentrations were measured in plasma epithelial lining fluid elf and lung tissue pd studies with these mice were undertaken with eight s aureus isolates two MSSA strains three hospitalacquired mrsa strains and three CA mrsa strains subcutaneous bpr doses of to mgkg of body weightday were administered and the NC in the number of log cfuml in lungs was evaluated after h of therapy the pd profile was characterized by using the free drug exposures f determined from the following parameters the percentage of time that the concentration was greater than the mic t mic the Cmax in serummic and the area under the concentrationtime curvemic the bpr pk parameters were linear over the dose range studied in plasma and the elf concentrations ranged from to of the free plasma concentration ft mic was the parameter that best correlated with tau against a diverse array of s aureus isolates in this mu pneumonia model the effective dose ed ed and stasis exposures appeared to be similar among the isolates studied bpr exerted maximal antibacterial effects when ft mic ranged from to regardless of the phenotypic profile of resistance to betalactam fluoroquinolone erythromycin clindamycin or TCs\\nLABEL: methicillinsusceptible s aureus'),\n",
              " Document(metadata={'row': 1626, 'source': 'medal.csv'}, page_content=': 1626\\nTEXT: despite improvements to the death notification form dnf used in south africa sa the quality of causeofdeath information remains suboptimal to address these inadequacies the government ran a trainthetrainer programme on completion of the dnf targeting doctors in public sector hospitals training materials were developed and workshops were held in all provinces this article reflects on the lessons learnt from the training and highlights issues that need to be addressed to improve medical certification and causeofdeath data in sa the dnf should be completed truthfully and accurately and confidentiality of the information on the form should be maintained the underlying cause of death should be entered on the lowest completed line in the causeofdeath section and if appropriate hiv should be entered here exclusion clauses for hiv in life insurance policies with association of savings and investments south africa companies were scrapped in interactive workshops provide a good learning environment but are logistically challenging more use should be made of online training resources particularly with CPD accreditation and helpline support in addition training in the completion of the dnf should become part of the curriculum in all medical schools and part of the orientation of interns and community service doctors in all facilities\\nLABEL: continuing professional development'),\n",
              " Document(metadata={'row': 1626, 'source': 'medal.csv'}, page_content=': 1626\\nTEXT: despite improvements to the death notification form dnf used in south africa sa the quality of causeofdeath information remains suboptimal to address these inadequacies the government ran a trainthetrainer programme on completion of the dnf targeting doctors in public sector hospitals training materials were developed and workshops were held in all provinces this article reflects on the lessons learnt from the training and highlights issues that need to be addressed to improve medical certification and causeofdeath data in sa the dnf should be completed truthfully and accurately and confidentiality of the information on the form should be maintained the underlying cause of death should be entered on the lowest completed line in the causeofdeath section and if appropriate hiv should be entered here exclusion clauses for hiv in life insurance policies with association of savings and investments south africa companies were scrapped in interactive workshops provide a good learning environment but are logistically challenging more use should be made of online training resources particularly with CPD accreditation and helpline support in addition training in the completion of the dnf should become part of the curriculum in all medical schools and part of the orientation of interns and community service doctors in all facilities\\nLABEL: continuing professional development')]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Проверка работы поиска:\n",
        "retriever.invoke(\"ceftobiprole bpr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задание 3: Prompt Engineering. Создание Prompt Template (3 балла)\n",
        "\n",
        "С помощью синтаксиса `jinja2` настройте шаблон для prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Пример использования PromptTemplate:\n",
        "\n",
        "```python\n",
        "PromptTemplate(template=tokenizer.chat_template, template_format='jinja2', input_variables=['content'])\n",
        "```\n",
        "\n",
        "*Советы*:\n",
        "- Prompt Templates можно посмотреть на https://github.com/chujiezheng/chat_templates и [replicate.com](www.replicate.com). Например, для LLama 3 они тут:\n",
        "    - https://replicate.com/meta/meta-llama-3-70b-instruct\n",
        "    - https://github.com/chujiezheng/chat_templates/blob/main/chat_templates/llama-3-chat.jinja\n",
        "\n",
        "```\n",
        "\"\"\"LLama 3 template:\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['SYSTEM_PROMPT', 'context', 'question'] input_types={} partial_variables={} template='<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{SYSTEM_PROMPT}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nВопрос: {question}\\n\\nКонтекст: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nОтвет:'\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Ты - медицинский ассистент, который предоставляет точные и полезные ответы \n",
        "на вопросы о здоровье, используя предоставленную информацию. Будь профессиональным и сострадательным.\"\"\" #None  # TODO: Введите системный промпт\n",
        "\n",
        "USE_HISTORY = False\n",
        "if USE_HISTORY:\n",
        "    # BONUS: Задание с историей переписки, см. ниже\n",
        "    instruction = None  # TODO: задайте инструкцию с использованием истории переписки\n",
        "    prompt_template = None  # TODO: вставьте шаблон с историей переписки\n",
        "    prompt = PromptTemplate(input_variables=None,  # TODO: введите переменные\n",
        "                            template=prompt_template)\n",
        "else:\n",
        "    # instruction = None  # TODO: задайте инструкцию\n",
        "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{SYSTEM_PROMPT}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Вопрос: {question}\n",
        "\n",
        "Контекст: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Ответ:\"\"\"#None  # TODO: вставьте шаблон\n",
        "    prompt = PromptTemplate(input_variables=[\"question\", \"context\", \"SYSTEM_PROMPT\"],  # TODO: введите переменные\n",
        "                            template=prompt_template)\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задание 4: Создание Chain (LangChain pipeline) (2 балла)\n",
        "\n",
        "Соберите пайплайн, включающий следующие этапы:\n",
        "- **Feature Engineering** (Retrieval Augmentation)\n",
        "- **Препроцессинг** (Prompt Engineering)\n",
        "- **Модель LLM**\n",
        "- **Постпроцессинг**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough(), \"SYSTEM_PROMPT\": lambda x: SYSTEM_PROMPT}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Примеры вызова цепочки:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Добро пожаловать в наш бот-помощник по вопросам о здоровье! Я буду рад помочь тебе с любым вопросом, связанным со здоровьем. Пожалуйста, не забывайте, что я — лишь программа и не могу предоставлять медицинскую помощь в реальном времени. Я буду рад помочь тебе найти достоверную и полезную информацию по вопросам о здоровье. Не стесняйтесь спрашивать меня что-либо, связанное со здоровьем, и я буду рад помочь тебе. Если вы уже имеете ответ на свой вопрос, то пожалуйста, не стесняйтесь его делиться с нами и другими пользователями, которые могут быть заинтересованы в этой информации. Мы будем рады помочь тебе"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    4424.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4422.20 ms /   845 tokens (    5.23 ms per token,   191.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8601.75 ms /   255 runs   (   33.73 ms per token,    29.65 tokens per second)\n",
            "llama_perf_context_print:       total time =   13384.66 ms /  1100 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Добро пожаловать в наш бот-помощник по вопросам о здоровье! Я буду рад помочь тебе с любым вопросом, связанным со здоровьем. Пожалуйста, не забывайте, что я — лишь программа и не могу предоставлять медицинскую помощь в реальном времени. Я буду рад помочь тебе найти достоверную и полезную информацию по вопросам о здоровье. Не стесняйтесь спрашивать меня что-либо, связанное со здоровьем, и я буду рад помочь тебе. Если вы уже имеете ответ на свой вопрос, то пожалуйста, не стесняйтесь его делиться с нами и другими пользователями, которые могут быть заинтересованы в этой информации. Мы будем рады помочь тебе\n"
          ]
        }
      ],
      "source": [
        "result1 = chain.invoke('How to treat pneumonia?')  # Пример запроса\n",
        "print(result1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 129 prefix-match hit, remaining 1417 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ": 2\n",
            "TEXT: ceftobiprole bpr is an investigational cephalosporin with activity against staphylococcus aureus including methicillinresistant s aureus mrsa strains the pharmacodynamic pd profile of bpr against s aureus strains with a variety of susceptibility phenotypes in an immunocompromised murine pneumonia model was characterized the bpr mics of the test isolates ranged from to mugml pharmacokinetic pk studies were conducted with infected neutropenic balbc mice and the bpr concentrations were measured in plasma epithelial lining fluid elf and lung tissue pd studies with these mice were undertaken with eight s aureus isolates two MSSA strains three hospitalacquired mrsa strains and three CA mrsa strains subcutaneous bpr doses of to mgkg of body weightday were administered and the NC in the number of log cfuml in lungs was evaluated after h of therapy the pd profile was characterized by using the free drug exposures f determined from the following parameters the percentage of time that the concentration"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    4424.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4933.23 ms /  1417 tokens (    3.48 ms per token,   287.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9267.50 ms /   255 runs   (   36.34 ms per token,    27.52 tokens per second)\n",
            "llama_perf_context_print:       total time =   14543.97 ms /  1672 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ": 2\n",
            "TEXT: ceftobiprole bpr is an investigational cephalosporin with activity against staphylococcus aureus including methicillinresistant s aureus mrsa strains the pharmacodynamic pd profile of bpr against s aureus strains with a variety of susceptibility phenotypes in an immunocompromised murine pneumonia model was characterized the bpr mics of the test isolates ranged from to mugml pharmacokinetic pk studies were conducted with infected neutropenic balbc mice and the bpr concentrations were measured in plasma epithelial lining fluid elf and lung tissue pd studies with these mice were undertaken with eight s aureus isolates two MSSA strains three hospitalacquired mrsa strains and three CA mrsa strains subcutaneous bpr doses of to mgkg of body weightday were administered and the NC in the number of log cfuml in lungs was evaluated after h of therapy the pd profile was characterized by using the free drug exposures f determined from the following parameters the percentage of time that the concentration\n"
          ]
        }
      ],
      "source": [
        "result2 = chain.invoke('Tell in details what is ceftobiprole bpr?')  # Пример запроса\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Бонус (2 балла): Добавление истории переписки\n",
        "\n",
        "**Подсказка:** Используйте `langchain.memory.ConversationBufferMemory` для интеграции истории переписки с ботом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "prompt_template = \"\"\"<s>[INST] <<SYS>>\n",
        "Ты - медицинский ассистент. Используй контекст для точного ответа.\n",
        "<</SYS>>\n",
        "\n",
        "Контекст: {context}\n",
        "История чата: {chat_history}\n",
        "Вопрос: {question} [/INST]</s>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\", 'chat_histoty'],\n",
        "    template=prompt_template\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", output_key='answer', return_messages=True)\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 322 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " What is the treatment for pneumonia?"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    4424.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2470.93 ms /   323 tokens (    7.65 ms per token,   130.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     280.96 ms /     9 runs   (   31.22 ms per token,    32.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1518.25 ms /   332 tokens\n",
            "/opt/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 1 prefix-match hit, remaining 1017 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1261\n",
            "\n",
            "Ответ: The treatment for pneumonia typically involves antibiotics to combat the bacterial infection. In some cases, additional treatments such as oxygen therapy, fluid management, and nutritional support may be necessary. The specific treatment approach depends on the severity of the pneumonia, the underlying cause of the infection, and the patient's overall health status.\n",
            "\n",
            "Additionally, in the context of the given text, Selective Decontamination (SD) was used as an additional preventive measure to reduce the risk of pneumonia in patients undergoing cardiac surgery in an Intensive Care Unit (ICU). The SD regimen included polymyxin gentamicin and nystatin given as an oral paste and as a solution, along with standard antacid or histamine blocker AS ulcer prophylaxis.\n",
            "\n",
            "It is important to note that while the SD approach was effective in reducing the incidence of pneumonia, it also came with some risks, including the potential for antibiotic resistance and the risk of disruption of the normal gut microbiome. Therefore, the decision to use SD as a preventive measure should be made on a case-by"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    4424.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3363.60 ms /  1017 tokens (    3.31 ms per token,   302.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8797.88 ms /   255 runs   (   34.50 ms per token,    28.98 tokens per second)\n",
            "llama_perf_context_print:       total time =   12579.58 ms /  1272 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1261\n",
            "\n",
            "Ответ: The treatment for pneumonia typically involves antibiotics to combat the bacterial infection. In some cases, additional treatments such as oxygen therapy, fluid management, and nutritional support may be necessary. The specific treatment approach depends on the severity of the pneumonia, the underlying cause of the infection, and the patient's overall health status.\n",
            "\n",
            "Additionally, in the context of the given text, Selective Decontamination (SD) was used as an additional preventive measure to reduce the risk of pneumonia in patients undergoing cardiac surgery in an Intensive Care Unit (ICU). The SD regimen included polymyxin gentamicin and nystatin given as an oral paste and as a solution, along with standard antacid or histamine blocker AS ulcer prophylaxis.\n",
            "\n",
            "It is important to note that while the SD approach was effective in reducing the incidence of pneumonia, it also came with some risks, including the potential for antibiotic resistance and the risk of disruption of the normal gut microbiome. Therefore, the decision to use SD as a preventive measure should be made on a case-by\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.invoke({\"question\": \"How to treat pneumonia?\"})['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 597 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Which antibiotics are the most effective for treating pneumonia?\n",
            "\n",
            "Translation: What antibiotics have the greatest effectiveness in treating pneumonia?"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    4424.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2107.60 ms /   597 tokens (    3.53 ms per token,   283.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1044.90 ms /    32 runs   (   32.65 ms per token,    30.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    3200.47 ms /   629 tokens\n",
            "/opt/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 1 prefix-match hit, remaining 2111 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The most effective antibiotics for treating pneumonia depend on the specific bacterial cause and its susceptibility patterns. However, some commonly used antibiotics with broad-spectrum activity against common bacterial pathogens causing community-acquired or hospital-acquired pneumonia include:\n",
            "\n",
            "1. Macrolides (such as azithromycin, clarithromycin)\n",
            "2. Fluoroquinolones (such as levofloxacin, moxifloxacin)\n",
            "3. Beta-lactams (such as ampicillin, piperacillin/tazobactam) in combination with a macrolide or fluoroquinolone to enhance coverage against atypical bacteria and streptococci.\n",
            "\n",
            "It is important to note that the choice of antibiotics should be based on susceptibility testing results and local resistance patterns, as well as patient factors such as comorbidities, allergies, and drug interactions. Additionally, adherence to recommended dosing regimens and monitoring for potential side effects or toxicities are crucial aspects of effective and safe antimicrobial therapy for pneumonia."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    4424.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7210.31 ms /  2111 tokens (    3.42 ms per token,   292.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9871.81 ms /   253 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_perf_context_print:       total time =   17378.14 ms /  2364 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The most effective antibiotics for treating pneumonia depend on the specific bacterial cause and its susceptibility patterns. However, some commonly used antibiotics with broad-spectrum activity against common bacterial pathogens causing community-acquired or hospital-acquired pneumonia include:\n",
            "\n",
            "1. Macrolides (such as azithromycin, clarithromycin)\n",
            "2. Fluoroquinolones (such as levofloxacin, moxifloxacin)\n",
            "3. Beta-lactams (such as ampicillin, piperacillin/tazobactam) in combination with a macrolide or fluoroquinolone to enhance coverage against atypical bacteria and streptococci.\n",
            "\n",
            "It is important to note that the choice of antibiotics should be based on susceptibility testing results and local resistance patterns, as well as patient factors such as comorbidities, allergies, and drug interactions. Additionally, adherence to recommended dosing regimens and monitoring for potential side effects or toxicities are crucial aspects of effective and safe antimicrobial therapy for pneumonia.\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.invoke({\"question\": \"Which antibiotics are the most effective against it?\"})['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
